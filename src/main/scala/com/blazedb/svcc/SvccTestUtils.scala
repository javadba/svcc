package com.blazedb.svccimport org.apache.spark.mllib.linalg.{Vector, Vectors}/** * SvccTestUtils * * Created by javadba@gmail.com on 10/10/14 */object SvccTestUtils {  import org.apache.spark.SparkContext  import org.apache.spark.mllib.random.RandomRDDs._  // Generate a random double RDD that contains 1 million i.i.d. values drawn from the  // standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.  // Then apply a transform to get a random double RDD following `N(1, 4)`.  def normal(sc: SparkContext, N: Long) = normalRDD(sc, N, 10).map {    x =>      1.0 + 2.0 * x  }  import org.apache.spark.mllib.stat._  import org.apache.spark.rdd.RDD  def getSummary(rdd: RDD[Vector]): MultivariateStatisticalSummary = {    // Coming in 1.2.0 !  rdd.treeAggregate(new MultivariateOnlineSummarizer)(    rdd.aggregate(new MultivariateOnlineSummarizer)(      (aggregator, data) => aggregator.add(data),      (aggregator1, aggregator2) => aggregator1.merge(aggregator2))  }  def printSummary(summ: MultivariateStatisticalSummary) = {    println(s"Count: ${summ.count}\nMean: ${summ.mean}\nMax: ${summ.max}\nVar: ${summ.variance}")  }  def genVectorRdd(sc: SparkContext, n: Int) = {    import java.util.Random    val xrand = new Random(37)    val yrand = new Random(41)    val zrand = new Random(43)    val SignalScale = 5.0    val NoiseScale = 8.0    val npoints = 50000    val XScale = SignalScale / npoints    val YScale = 2.0 * SignalScale / npoints    val ZScale = 3.0 * SignalScale / npoints    val randRdd = sc.parallelize({      for (p <- Range(0, npoints))      yield        (NoiseScale * xrand.nextGaussian + XScale * p,          NoiseScale * yrand.nextGaussian + YScale * p,          NoiseScale * zrand.nextGaussian + ZScale * p)    }.toSeq, 2)    val vecs = randRdd.map {      case (x, y, z) =>        Vectors.dense(Array(x, y, z))    }    val summary = getSummary(vecs)    printSummary(summary)    vecs  }}